{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Old cell\n",
    "```\n",
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "# Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install text-unidecode\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "# Install TorchAudio\n",
    "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PretrainedModelInfo(\n",
       " \tpretrained_model_name=speakerverification_speakernet,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:speakerverification_speakernet,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/speakerverification_speakernet/versions/1.16.0/files/speakerverification_speakernet.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=ecapa_tdnn,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:ecapa_tdnn,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/ecapa_tdnn/versions/1.16.0/files/ecapa_tdnn.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=titanet_large,\n",
       " \tdescription=For details about this model, please visit https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/titanet_large,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/titanet_large/versions/v1/files/titanet-l.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=langid_ambernet,\n",
       " \tdescription=For details about this model, please visit https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/langid_ambernet,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/langid_ambernet/versions/1.12.0/files/ambernet.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=titanet_small,\n",
       " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:titanet_small,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/titanet_small/versions/1.19.0/files/titanet-s.nemo\n",
       " )]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "nemo_asr.models.EncDecSpeakerLabelModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:11.622187Z",
     "start_time": "2024-03-09T09:44:11.617810Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new cell\n",
    "BRANCH = 'main'\n",
    "\n",
    "# from https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html\n",
    "\n",
    "model_name = 'ecapa_tdnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDzak_FIB9LS"
   },
   "source": [
    "# **SPEAKER RECOGNITION** \n",
    "Speaker Recognition (SR) is a broad research area that solves two major tasks: speaker identification (who is speaking?) and\n",
    "speaker verification (is the speaker who they claim to be?). In this work, we focus on text-independent speaker recognition when the identity of the speaker is based on how the speech is spoken,\n",
    "not necessarily in what is being said. Typically such SR systems operate on unconstrained speech utterances,\n",
    "which are converted into fixed-length vectors, called speaker embeddings. Speaker embeddings are also used in\n",
    "automatic speech recognition (ASR) and speech synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydqmdcDxCeXb"
   },
   "source": [
    "In this tutorial, we shall first train these embeddings on speaker-related datasets, and then get speaker embeddings from a pretrained network for a new dataset. Since Google Colab has very slow read-write speeds, I'll be demonstrating this tutorial using [an4](http://www.speech.cs.cmu.edu/databases/an4/). \n",
    "\n",
    "Instead, if you'd like to try on a bigger dataset like [hi-mia](https://arxiv.org/abs/1912.01231) use the [get_hi-mia-data.py](https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing/speaker_tasks/get_hi-mia_data.py) script to download the necessary files, extract them, and resample to 16Khz if any of these samples are not at 16Khz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:14.141114Z",
     "start_time": "2024-03-09T09:44:11.819628Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vqUBayc_Ctcr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eyal/git/hakol/notebooks\n",
      "******\n",
      "Tarfile already exists.\n",
      "Converting .sph to .wav...\n",
      "Finished conversion.\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "NEMO_ROOT = os.getcwd()\n",
    "print(NEMO_ROOT)\n",
    "import glob\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "data_dir = os.path.join(NEMO_ROOT,'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the dataset. This will take a few moments...\n",
    "print(\"******\")\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"Dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"Tarfile already exists.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'\n",
    "\n",
    "# Untar and convert .sph to .wav (using sox)\n",
    "tar = tarfile.open(an4_path)\n",
    "tar.extractall(path=data_dir)\n",
    "\n",
    "print(\"Converting .sph to .wav...\")\n",
    "sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "for sph_path in sph_list:\n",
    "    wav_path = sph_path[:-4] + '.wav'\n",
    "    cmd = [\"sox\", sph_path, wav_path]\n",
    "    subprocess.run(cmd)\n",
    "print(\"Finished conversion.\\n******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5PrWzkiDbHy"
   },
   "source": [
    "Since an4 is not designed for speaker recognition, this facilitates the opportunity to demonstrate how you can generate manifest files that are necessary for training. These methods can be applied to any dataset to get similar training manifest files. \n",
    "\n",
    "First, create a list file which has all the wav files with absolute paths for each of the train, dev, and test set. This can be easily done by the `find` bash command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:14.333675Z",
     "start_time": "2024-03-09T09:44:14.142443Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vnrUh3vuDSRN"
   },
   "outputs": [],
   "source": [
    "!find {data_dir}/an4/wav/an4_clstk  -iname \"*.wav\" > data/an4/wav/an4_clstk/train_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhWVg2QoDhL3"
   },
   "source": [
    "Let's look at the first 3 lines of filelist text file for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:14.522065Z",
     "start_time": "2024-03-09T09:44:14.334802Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BfnMK302Du20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/mcen/an126-mcen-b.wav\n",
      "/home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/mcen/cen6-mcen-b.wav\n",
      "/home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/mcen/an130-mcen-b.wav\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 {data_dir}/an4/wav/an4_clstk/train_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9L9Tl0XDw5Z"
   },
   "source": [
    "Since we created the list text file for the train, we use `filelist_to_manifest.py` to convert this text file to a manifest file and then optionally split the files to train \\& dev for evaluating the models during training by using the `--split` flag. We wouldn't be needing the `--split` option for the test folder. \n",
    "Accordingly please mention the `id` number, which is the field num separated by `/` to be considered as the speaker label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LYwHAr1G8hp"
   },
   "source": [
    "After the download and conversion, your `data` folder should contain directories with manifest files as:\n",
    "\n",
    "* `data/<path>/train.json`\n",
    "* `data/<path>/dev.json` \n",
    "* `data/<path>/train_all.json` \n",
    "\n",
    "Each line in the manifest file describes a training sample - `audio_filepath` contains the path to the wav file, `duration` it's duration in seconds, and `label` is the speaker class label:\n",
    "\n",
    "`{\"audio_filepath\": \"<absolute path to dataset>data/an4/wav/an4test_clstk/menk/cen4-menk-b.wav\", \"duration\": 3.9, \"label\": \"menk\"}` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:19.632181Z",
     "start_time": "2024-03-09T09:44:14.523846Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mpAv77JoD98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 948/948 [00:00<00:00, 3892.72it/s]\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/train_all_manifest.json\n",
      "100%|██████████████████████████████████████| 948/948 [00:00<00:00, 89839.36it/s]\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/all_manifest.json\n",
      "number of train samples after split:  853\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/train.json\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/dev.json\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('scripts'):\n",
    "  print(\"Downloading necessary scripts\")\n",
    "  !mkdir -p scripts/speaker_tasks\n",
    "  !wget -P scripts/speaker_tasks/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/speaker_tasks/filelist_to_manifest.py\n",
    "!python {NEMO_ROOT}/scripts/speaker_tasks/filelist_to_manifest.py --filelist {data_dir}/an4/wav/an4_clstk/train_all.txt --id -2 --out {data_dir}/an4/wav/an4_clstk/all_manifest.json --split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kPCmx5DHvY5"
   },
   "source": [
    "Generate the list text file for the test folder and then convert it to a manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:24.906071Z",
     "start_time": "2024-03-09T09:44:19.633438Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "nMd24GVaFBwr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 130/130 [00:00<00:00, 346.95it/s]\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4test_clstk/test_all_manifest.json\n",
      "100%|██████████████████████████████████████| 130/130 [00:00<00:00, 32040.16it/s]\n",
      "wrote /home/eyal/git/hakol/notebooks/data/an4/wav/an4test_clstk/test.json\n"
     ]
    }
   ],
   "source": [
    "!find {data_dir}/an4/wav/an4test_clstk  -iname \"*.wav\" > {data_dir}/an4/wav/an4test_clstk/test_all.txt\n",
    "!python {NEMO_ROOT}/scripts/speaker_tasks/filelist_to_manifest.py --filelist {data_dir}/an4/wav/an4test_clstk/test_all.txt --id -2 --out {data_dir}/an4/wav/an4test_clstk/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5FPmxUkGakD"
   },
   "source": [
    "## Path to manifest files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:24.910209Z",
     "start_time": "2024-03-09T09:44:24.907054Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vo-VnYPtJO_v"
   },
   "outputs": [],
   "source": [
    "train_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/train.json')\n",
    "validation_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/dev.json')\n",
    "test_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyDVdtjAL2__"
   },
   "source": [
    "As the goal of most speaker-related systems is to get good speaker level embeddings that could help distinguish from\n",
    "other speakers, we shall first train these embeddings in an end-to-end\n",
    "manner optimizing the [TitaNet](https://arxiv.org/pdf/2110.04410.pdf) model.\n",
    "We modify the decoder to get these fixed-size embeddings irrespective of the length of the input audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJtU_GEdMUUo"
   },
   "source": [
    "# Training\n",
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All the following steps are just for explanation of each section, but one can use the provided [training script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco.py) to launch training in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:28.315038Z",
     "start_time": "2024-03-09T09:44:24.911218Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o1ojB0cZMSmv"
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "# NeMo's ASR collection - This collection contains complete ASR models and\n",
    "# building blocks (modules) for ASR\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5Zho11LNAFJ"
   },
   "source": [
    "## Model Configuration \n",
    "The TitaNet model is defined in a config file which declares multiple important sections.\n",
    "\n",
    "They are:\n",
    "\n",
    "1) model: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets, and any other related information\n",
    "\n",
    "2) trainer: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.035669Z",
     "start_time": "2024-03-09T09:44:28.315962Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6HQtZfKnMhpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: ECAPA_TDNN\n",
      "model:\n",
      "  sample_rate: 16000\n",
      "  train_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: ${model.sample_rate}\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: null\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: ${model.sample_rate}\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "  validation_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: ${model.sample_rate}\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "  preprocessor:\n",
      "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "    normalize: per_feature\n",
      "    window_size: 0.025\n",
      "    sample_rate: ${model.sample_rate}\n",
      "    window_stride: 0.01\n",
      "    window: hann\n",
      "    features: 80\n",
      "    n_fft: 512\n",
      "    frame_splicing: 1\n",
      "    dither: 1.0e-05\n",
      "    stft_conv: false\n",
      "  spec_augment:\n",
      "    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "    freq_masks: 3\n",
      "    freq_width: 4\n",
      "    time_masks: 5\n",
      "    time_width: 0.03\n",
      "  encoder:\n",
      "    _target_: nemo.collections.asr.modules.ECAPAEncoder\n",
      "    feat_in: ${model.preprocessor.features}\n",
      "    filters:\n",
      "    - 1024\n",
      "    - 1024\n",
      "    - 1024\n",
      "    - 1024\n",
      "    - 3072\n",
      "    kernel_sizes:\n",
      "    - 5\n",
      "    - 3\n",
      "    - 3\n",
      "    - 3\n",
      "    - 1\n",
      "    dilations:\n",
      "    - 1\n",
      "    - 1\n",
      "    - 1\n",
      "    - 1\n",
      "    - 1\n",
      "    scale: 8\n",
      "  decoder:\n",
      "    _target_: nemo.collections.asr.modules.SpeakerDecoder\n",
      "    feat_in: 3072\n",
      "    num_classes: 7205\n",
      "    pool_mode: attention\n",
      "    emb_sizes: 192\n",
      "  loss:\n",
      "    _target_: nemo.collections.asr.losses.angularloss.AngularSoftmaxLoss\n",
      "    scale: 30\n",
      "    margin: 0.2\n",
      "  optim:\n",
      "    name: sgd\n",
      "    lr: 0.08\n",
      "    weight_decay: 0.0002\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      min_lr: 0.0001\n",
      "trainer:\n",
      "  devices: 1\n",
      "  max_epochs: 250\n",
      "  max_steps: -1\n",
      "  num_nodes: 1\n",
      "  accelerator: gpu\n",
      "  strategy: ddp\n",
      "  accumulate_grad_batches: 1\n",
      "  deterministic: false\n",
      "  enable_checkpointing: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: ${name}\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This line will print the entire config of sample TitaNet model\n",
    "os.makedirs('conf', exist_ok=True)\n",
    "url = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/speaker_tasks/recognition/conf/{model_name}.yaml\"\n",
    "wget.download(url, out=f\"./conf/{model_name}.yaml\")\n",
    "\n",
    "MODEL_CONFIG = os.path.join(NEMO_ROOT,f'conf/{model_name}.yaml')\n",
    "config = OmegaConf.load(MODEL_CONFIG)\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtbXN-cFOwxi"
   },
   "source": [
    "## Setting up the datasets within the config\n",
    "If you'll notice, there are a few config dictionaries called train_ds, validation_ds and test_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.041314Z",
     "start_time": "2024-03-09T09:44:29.037095Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NPBIf1jmNgjn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifest_filepath: ???\n",
      "sample_rate: ${model.sample_rate}\n",
      "labels: null\n",
      "batch_size: 64\n",
      "shuffle: true\n",
      "augmentor:\n",
      "  noise:\n",
      "    manifest_path: null\n",
      "    prob: 0.5\n",
      "    min_snr_db: 0\n",
      "    max_snr_db: 15\n",
      "  speed:\n",
      "    prob: 0.5\n",
      "    sr: ${model.sample_rate}\n",
      "    resample_type: kaiser_fast\n",
      "    min_speed_rate: 0.95\n",
      "    max_speed_rate: 1.05\n",
      "\n",
      "manifest_filepath: ???\n",
      "sample_rate: ${model.sample_rate}\n",
      "labels: null\n",
      "batch_size: 128\n",
      "shuffle: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.model.train_ds))\n",
    "print(OmegaConf.to_yaml(config.model.validation_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLIjKOMUP0YE"
   },
   "source": [
    "You will often notice that some configs have ??? in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
    "\n",
    "Let's add the paths to the manifests to the config above\n",
    "Also, since an4 dataset doesn't have a test set of the same speakers used in training, we will use validation manifest as test manifest for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.045500Z",
     "start_time": "2024-03-09T09:44:29.043510Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TSotpjL_O2BN"
   },
   "outputs": [],
   "source": [
    "config.model.train_ds.manifest_filepath = train_manifest\n",
    "config.model.validation_ds.manifest_filepath = validation_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since we are training speaker embedding extractor model for verification we do not add test_ds dataset. To include it add it to config and replace manifest file as \n",
    "`config.model.test_ds.manifest_filepath = test_manifest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xy6_Lf6fW9aJ"
   },
   "source": [
    "Also as we are training on an4 dataset, there are 74 speaker labels in training, and we need to set this in the decoder config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.049202Z",
     "start_time": "2024-03-09T09:44:29.046090Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-B96tFTnW8Yh"
   },
   "outputs": [],
   "source": [
    "config.model.decoder.num_classes = 74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83pHBRDpQTF0"
   },
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
    "\n",
    "Let us first instantiate a Trainer object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.054462Z",
     "start_time": "2024-03-09T09:44:29.049819Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GWzGJoHMQQnG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.058750Z",
     "start_time": "2024-03-09T09:44:29.055383Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WIYf4-KFQYHl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "devices: 1\n",
      "max_epochs: 250\n",
      "max_steps: -1\n",
      "num_nodes: 1\n",
      "accelerator: gpu\n",
      "strategy: ddp\n",
      "accumulate_grad_batches: 1\n",
      "deterministic: false\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "gradient_clip_val: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.101192Z",
     "start_time": "2024-03-09T09:44:29.059460Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aXuSMYMNQeW7"
   },
   "outputs": [],
   "source": [
    "# Let us modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "config.trainer.devices = 1\n",
    "config.trainer.accelerator = accelerator\n",
    "\n",
    "# Reduces maximum number of epochs to 5 for quick demonstration\n",
    "config.trainer.max_epochs = 10\n",
    "\n",
    "# Remove distributed training flags\n",
    "config.trainer.strategy = 'auto'\n",
    "\n",
    "# Remove augmentations\n",
    "config.model.train_ds.augmentor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n",
    "        print(\"embeddings\")\n",
    "        return super().on_train_epoch_end(trainer, pl_module)\n",
    "        \n",
    "    # def on_init_start(self, trainer):\n",
    "    #     print(\"Starting to init trainer!\")\n",
    "\n",
    "    # def on_init_end(self, trainer):\n",
    "    #     print(\"trainer is init now\")\n",
    "\n",
    "    # def on_train_end(self, trainer, pl_module):\n",
    "    #     print(\"do something when training ends\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.217426Z",
     "start_time": "2024-03-09T09:44:29.102628Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pBq3eCLwQhCy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**config.trainer, callbacks=[MyPrintingCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xHq_rcmQiry"
   },
   "source": [
    "## Setting up a NeMo Experiment\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.268364Z",
     "start_time": "2024-03-09T09:44:29.218617Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DMm8MPYfQsCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:21:57 exp_manager:396] Experiments will be logged at /home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57\n",
      "[NeMo I 2024-03-11 08:21:57 exp_manager:842] TensorboardLogger has been set up\n",
      "/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57\n"
     ]
    }
   ],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "log_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "# The log_dir provides a path to the current logging directory for easy access\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQQMlXmLQ7h1"
   },
   "source": [
    "## Building the TitaNet Model\n",
    "TitaNet is a speaker embedding extractor model that can be used for speaker identification tasks - it generates one label for the entire provided audio stream. Therefore we encapsulate it inside the EncDecSpeakerLabelModel as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.474846Z",
     "start_time": "2024-03-09T09:44:29.269448Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "E_KY_s5LROYf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:21:57 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:446] Dataset loaded with 853 items, total duration of  0.63 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:448] # 853 files loaded accounting to # 74 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:21:57 label_models:189] Total number of 74 found in all the manifest files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:21:57 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:446] Dataset loaded with 853 items, total duration of  0.63 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:448] # 853 files loaded accounting to # 74 labels\n",
      "[NeMo I 2024-03-11 08:21:57 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:446] Dataset loaded with 95 items, total duration of  0.07 hours.\n",
      "[NeMo I 2024-03-11 08:21:57 collections:448] # 95 files loaded accounting to # 74 labels\n",
      "[NeMo I 2024-03-11 08:21:57 features:289] PADDING: 16\n"
     ]
    }
   ],
   "source": [
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AphpMhkSVdU"
   },
   "source": [
    "Before we begin training, let us first create a Tensorboard visualization to monitor progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:29.480351Z",
     "start_time": "2024-03-09T09:44:29.476022Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BUnDpe_5SbDR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Or8g1cksSf8C"
   },
   "source": [
    "As any NeMo model is inherently a PyTorch Lightning Model, it can easily be trained in a single line - trainer.fit(model)!\n",
    "Below we see that the model begins to get modest scores on the validation set after just 5 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:35.797840Z",
     "start_time": "2024-03-09T09:44:29.481053Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HvYhsOWuSpL_",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:21:58 modelPT:723] Optimizer config = SGD (\n",
      "    Parameter Group 0\n",
      "        dampening: 0\n",
      "        differentiable: False\n",
      "        foreach: None\n",
      "        lr: 0.08\n",
      "        maximize: False\n",
      "        momentum: 0\n",
      "        nesterov: False\n",
      "        weight_decay: 0.0002\n",
      "    )\n",
      "[NeMo I 2024-03-11 08:21:58 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f3d95056e90>\" \n",
      "    will be used during training (effective maximum steps = 140) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1\n",
      "    min_lr: 0.0001\n",
      "    max_steps: 140\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                              | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | loss              | AngularSoftmaxLoss                | 0     \n",
      "1 | eval_loss         | AngularSoftmaxLoss                | 0     \n",
      "2 | _accuracy         | TopKClassificationAccuracy        | 0     \n",
      "3 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
      "4 | encoder           | ECAPAEncoder                      | 18.1 M\n",
      "5 | decoder           | SpeakerDecoder                    | 2.8 M \n",
      "6 | _macro_accuracy   | MulticlassAccuracy                | 0     \n",
      "7 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
      "------------------------------------------------------------------------\n",
      "20.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.9 M    Total params\n",
      "83.725    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c9ceed13e84d33a705ae4b3a547d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:21:58 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-03-11 08:21:59 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56539b9af2f4339827d7abe30ef687a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:21:59 preemption:56] Preemption requires torch distributed to be initialized, disabling preemption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:21:59 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf0ab1d48674a299c01269cd649d15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 14: 'val_loss' reached 12.67487 (best 12.67487), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=12.6749-epoch=0.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04996f4fe1b41e383693a11b53a8093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 28: 'val_loss' reached 11.54905 (best 11.54905), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=11.5490-epoch=1.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee387465e32043c080c35ec2b6294430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 42: 'val_loss' reached 9.75225 (best 9.75225), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=9.7523-epoch=2.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04a9e5009294fefabcb2b846a9e7e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 56: 'val_loss' reached 7.86298 (best 7.86298), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=7.8630-epoch=3.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf66043b9d6e4622bb7f683f4d12faee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 70: 'val_loss' reached 7.29584 (best 7.29584), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=7.2958-epoch=4.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb513bdc87a54ce9b3744568e48e347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 84: 'val_loss' reached 6.67396 (best 6.67396), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.6740-epoch=5.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e4f75391964f449fd8d3e2aab03e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 98: 'val_loss' reached 6.55203 (best 6.55203), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.5520-epoch=6.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99914f8e2e047dd938b74dbb7a020fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 112: 'val_loss' reached 6.19838 (best 6.19838), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1984-epoch=7.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc50843ac499473494ffce0783e2ae38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 126: 'val_loss' reached 6.10824 (best 6.10824), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1082-epoch=8.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d1b0f664eb4beeaca3ec1bb3766a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 140: 'val_loss' reached 6.11986 (best 6.10824), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1199-epoch=9.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(speaker_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSRACGt3UAYn"
   },
   "source": [
    "This config is not suited and designed for an4 so you may observe unstable val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvtVKO8FZsoe"
   },
   "source": [
    "If you have a test manifest file, we can easily compute test accuracy by running\n",
    "<pre><code>trainer.test(speaker_model, ckpt_path=None)\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlBwMsRdZfqg"
   },
   "source": [
    "## For Faster Training\n",
    "We can dramatically improve the time taken to train this model by using Multi GPU training along with Mixed Precision.\n",
    "\n",
    "### Trainer with a distributed backend:\n",
    "<pre><code>trainer = Trainer(devices=2, num_nodes=2, accelerator='gpu', strategy='auto')\n",
    "</code></pre>\n",
    "\n",
    "### Mixed precision:\n",
    "<pre><code>trainer = Trainer(amp_level='O1', precision=16)\n",
    "</code></pre>\n",
    "\n",
    "Of course, you can combine these flags as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcnWub9-0TW2"
   },
   "source": [
    "## Saving/Restoring a checkpoint\n",
    "There are multiple ways to save and load models in NeMo. Since all NeMo models are inherently Lightning Modules, we can use the standard way that PyTorch Lightning saves and restores models.\n",
    "\n",
    "NeMo also provides a more advanced model save/restore format, which encapsulates all the parts of the model that are required to restore that model for immediate use.\n",
    "\n",
    "In this example, we will explore both ways of saving and restoring models, but we will focus on the PyTorch Lightning method.\n",
    "\n",
    "## Saving and Restoring via PyTorch Lightning Checkpoints\n",
    "When using NeMo for training, it is advisable to utilize the exp_manager framework. It is tasked with handling checkpointing and logging (Tensorboard as well as WandB optionally!), as well as dealing with multi-node and multi-GPU logging.\n",
    "\n",
    "Since we utilized the exp_manager framework above, we have access to the directory where the checkpoints exist.\n",
    "\n",
    "exp_manager with the default settings will save multiple checkpoints for us -\n",
    "\n",
    "1) A few checkpoints from certain steps of training. They will have --val_loss= tags\n",
    "\n",
    "2) Checkpoints at the last epoch of training are denoted by --last.\n",
    "\n",
    "3) If the model finishes training, it will also have a --last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:35.802884Z",
     "start_time": "2024-03-09T09:44:35.798871Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QSLjq-edaPt_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1984-epoch=7.ckpt',\n",
       " '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1199-epoch=10-last.ckpt',\n",
       " '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1082-epoch=8.ckpt',\n",
       " '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1199-epoch=9.ckpt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us list all the checkpoints we have\n",
    "checkpoint_dir = os.path.join(log_dir, 'checkpoints')\n",
    "checkpoint_paths = list(glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\")))\n",
    "checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:35.807500Z",
     "start_time": "2024-03-09T09:44:35.803649Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BwltdVWXaroa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=6.1199-epoch=10-last.ckpt\n"
     ]
    }
   ],
   "source": [
    "final_checkpoint = list(filter(lambda x: \"-last.ckpt\" in x, checkpoint_paths))[0]\n",
    "print(final_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tGKKojs0fEh"
   },
   "source": [
    "\n",
    "## Restoring from a PyTorch Lightning checkpoint\n",
    "To restore a model using the LightningModule.load_from_checkpoint() class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.191136Z",
     "start_time": "2024-03-09T09:44:35.808342Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EgyP9cYVbFc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:23:02 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/train.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - fash\n",
      "    - fbbh\n",
      "    - fclc\n",
      "    - fejs\n",
      "    - ffmm\n",
      "    - fjam\n",
      "    - fjdn\n",
      "    - fjmd\n",
      "    - fkai\n",
      "    - fkdo\n",
      "    - flmm2\n",
      "    - flrp\n",
      "    - fmjc\n",
      "    - fmjd\n",
      "    - fnsv\n",
      "    - fplp\n",
      "    - fsaf2\n",
      "    - fsrb\n",
      "    - ftal\n",
      "    - ftmj\n",
      "    - fwxs\n",
      "    - mblb\n",
      "    - mblw\n",
      "    - mbmg\n",
      "    - mcel\n",
      "    - mcen\n",
      "    - mcfl\n",
      "    - mcrt\n",
      "    - mcsc\n",
      "    - mdcs\n",
      "    - mdcs2\n",
      "    - mdmc\n",
      "    - mdxn\n",
      "    - mdxs\n",
      "    - meab\n",
      "    - meht\n",
      "    - mema\n",
      "    - mewl\n",
      "    - mfaa\n",
      "    - mgah\n",
      "    - mjbh\n",
      "    - mjda\n",
      "    - mjdr\n",
      "    - mjes\n",
      "    - mjgk\n",
      "    - mjhp\n",
      "    - mjjs2\n",
      "    - mkdb\n",
      "    - mkem\n",
      "    - mmaf\n",
      "    - mmal\n",
      "    - mmap\n",
      "    - mmdg\n",
      "    - mmkw\n",
      "    - mmsh\n",
      "    - mmtm\n",
      "    - mnfe\n",
      "    - mnjl\n",
      "    - mrab\n",
      "    - mrcb\n",
      "    - mrjc2\n",
      "    - mrmg\n",
      "    - mscg2\n",
      "    - msct\n",
      "    - msjm\n",
      "    - msjr\n",
      "    - mskh\n",
      "    - msmn\n",
      "    - msrb\n",
      "    - mtcv\n",
      "    - mtje\n",
      "    - mtos\n",
      "    - mtxj\n",
      "    - mwhw\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    augmentor: null\n",
      "    \n",
      "[NeMo W 2024-03-11 08:23:02 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /home/eyal/git/hakol/notebooks/data/an4/wav/an4_clstk/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:02 features:289] PADDING: 16\n"
     ]
    }
   ],
   "source": [
    "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.load_from_checkpoint(final_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnZVMKZpbI_M"
   },
   "source": [
    "# Finetuning\n",
    "Since we don't have any new manifest file to finetune, I will demonstrate here by using the test manifest file we created earlier. \n",
    "an4 test dataset has a different set of speakers from the train set (total number: 10). And as we didn't split this dataset for validation I will use the same for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV9gInFwQ2F5"
   },
   "source": [
    "There are a couple of ways we can finetune a speaker recognition model. \n",
    "1. Finetuning using a pretrained model published on NGC. \n",
    "2. Finetuning from a PTL checkpoint. \n",
    "\n",
    "Since finetuning from a large pretrained model is more common, I shall use it to demonstrate finetuning procedure. In order to make finetuning step independent from training from scratch, we use another config. Here we shall use `titanet-finetune.yaml` config, that is created to show finetuning on pretrained titanet-large model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You may use [finetune-script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco_finetune.py) to launch training in the command line. Following is just a demonstration of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.619456Z",
     "start_time": "2024-03-09T09:44:36.191886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-11 08:23:02--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/recognition/conf/titanet-finetune.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4507 (4.4K) [text/plain]\n",
      "Saving to: ‘conf/titanet-finetune.yaml’\n",
      "\n",
      "titanet-finetune.ya 100%[===================>]   4.40K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-03-11 08:23:03 (29.4 MB/s) - ‘conf/titanet-finetune.yaml’ saved [4507/4507]\n",
      "\n",
      "name: TitaNet-Finetune\n",
      "sample_rate: 16000\n",
      "init_from_pretrained_model:\n",
      "  speaker_tasks:\n",
      "    name: titanet_large\n",
      "    include:\n",
      "    - preprocessor\n",
      "    - encoder\n",
      "    exclude:\n",
      "    - decoder.final\n",
      "model:\n",
      "  train_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      speed:\n",
      "        prob: 0.3\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "  validation_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "  model_defaults:\n",
      "    filters: 1024\n",
      "    repeat: 3\n",
      "    dropout: 0.1\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    kernel_size_factor: 1.0\n",
      "  preprocessor:\n",
      "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "    normalize: per_feature\n",
      "    window_size: 0.025\n",
      "    sample_rate: 16000\n",
      "    window_stride: 0.01\n",
      "    window: hann\n",
      "    features: 80\n",
      "    n_fft: 512\n",
      "    frame_splicing: 1\n",
      "    dither: 1.0e-05\n",
      "  encoder:\n",
      "    _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "    feat_in: 80\n",
      "    activation: relu\n",
      "    conv_mask: true\n",
      "    jasper:\n",
      "    - filters: ${model.model_defaults.filters}\n",
      "      repeat: 1\n",
      "      kernel:\n",
      "      - 3\n",
      "      stride:\n",
      "      - 1\n",
      "      dilation:\n",
      "      - 1\n",
      "      dropout: 0.0\n",
      "      residual: false\n",
      "      separable: ${model.model_defaults.separable}\n",
      "      se: ${model.model_defaults.se}\n",
      "      se_context_size: ${model.model_defaults.se_context_size}\n",
      "    - filters: ${model.model_defaults.filters}\n",
      "      repeat: ${model.model_defaults.repeat}\n",
      "      kernel:\n",
      "      - 7\n",
      "      stride:\n",
      "      - 1\n",
      "      dilation:\n",
      "      - 1\n",
      "      dropout: ${model.model_defaults.dropout}\n",
      "      residual: true\n",
      "      separable: ${model.model_defaults.separable}\n",
      "      se: ${model.model_defaults.se}\n",
      "      se_context_size: ${model.model_defaults.se_context_size}\n",
      "    - filters: ${model.model_defaults.filters}\n",
      "      repeat: ${model.model_defaults.repeat}\n",
      "      kernel:\n",
      "      - 11\n",
      "      stride:\n",
      "      - 1\n",
      "      dilation:\n",
      "      - 1\n",
      "      dropout: ${model.model_defaults.dropout}\n",
      "      residual: true\n",
      "      separable: ${model.model_defaults.separable}\n",
      "      se: ${model.model_defaults.se}\n",
      "      se_context_size: ${model.model_defaults.se_context_size}\n",
      "    - filters: ${model.model_defaults.filters}\n",
      "      repeat: ${model.model_defaults.repeat}\n",
      "      kernel:\n",
      "      - 15\n",
      "      stride:\n",
      "      - 1\n",
      "      dilation:\n",
      "      - 1\n",
      "      dropout: ${model.model_defaults.dropout}\n",
      "      residual: true\n",
      "      separable: ${model.model_defaults.separable}\n",
      "      se: ${model.model_defaults.se}\n",
      "      se_context_size: ${model.model_defaults.se_context_size}\n",
      "    - filters: 3072\n",
      "      repeat: 1\n",
      "      kernel:\n",
      "      - 1\n",
      "      stride:\n",
      "      - 1\n",
      "      dilation:\n",
      "      - 1\n",
      "      dropout: 0.0\n",
      "      residual: false\n",
      "      separable: ${model.model_defaults.separable}\n",
      "      se: ${model.model_defaults.se}\n",
      "      se_context_size: ${model.model_defaults.se_context_size}\n",
      "  decoder:\n",
      "    _target_: nemo.collections.asr.modules.SpeakerDecoder\n",
      "    feat_in: 3072\n",
      "    num_classes: ???\n",
      "    pool_mode: attention\n",
      "    emb_sizes: 192\n",
      "  loss:\n",
      "    _target_: nemo.collections.asr.losses.angularloss.AngularSoftmaxLoss\n",
      "    scale: 30\n",
      "    margin: 0.2\n",
      "  optim_param_groups:\n",
      "    encoder:\n",
      "      lr: 0.001\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0002\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      min_lr: 0.0\n",
      "trainer:\n",
      "  devices: 1\n",
      "  max_epochs: 10\n",
      "  max_steps: -1\n",
      "  num_nodes: 1\n",
      "  accelerator: gpu\n",
      "  strategy: ddp\n",
      "  deterministic: true\n",
      "  enable_checkpointing: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: TitaNet-Finetune\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P conf https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/speaker_tasks/recognition/conf/titanet-finetune.yaml\n",
    "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/titanet-finetune.yaml')\n",
    "finetune_config = OmegaConf.load(MODEL_CONFIG)\n",
    "print(OmegaConf.to_yaml(finetune_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 2, if one would like to finetune from a PTL checkpoint, `init_from_pretrained_model` in config should be replaced with `init_from_nemo_model` and need to provide the path to checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.623699Z",
     "start_time": "2024-03-09T09:44:36.620489Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HtXUWmYLQ0PJ"
   },
   "outputs": [],
   "source": [
    "test_manifest = os.path.join(data_dir,'an4/wav/an4test_clstk/test.json')\n",
    "finetune_config.model.train_ds.manifest_filepath = test_manifest\n",
    "finetune_config.model.validation_ds.manifest_filepath = test_manifest\n",
    "finetune_config.model.decoder.num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHy1zE1cTDZn"
   },
   "source": [
    "So we have set up the data and changed the decoder required for finetune, we now just need to create a trainer and start training with a smaller learning rate for fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.630523Z",
     "start_time": "2024-03-09T09:44:36.624354Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "nBmF6tQITSRl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: 1\n",
      "accelerator: gpu\n",
      "max_epochs: 5\n",
      "max_steps: -1\n",
      "num_nodes: 1\n",
      "accumulate_grad_batches: 1\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the new trainer object\n",
    "# Let us modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer_config = OmegaConf.create(dict(\n",
    "    devices=1,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=5,\n",
    "    max_steps=-1,  # computed at runtime if not set\n",
    "    num_nodes=1,\n",
    "    accumulate_grad_batches=1,\n",
    "    enable_checkpointing=False,  # Provided by exp_manager\n",
    "    logger=False,  # Provided by exp_manager\n",
    "    log_every_n_steps=1,  # Interval of logging.\n",
    "    val_check_interval=1.0,  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
    "))\n",
    "print(OmegaConf.to_yaml(trainer_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.646231Z",
     "start_time": "2024-03-09T09:44:36.631152Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bRz-8-xzUHKZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer_finetune = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOwHTkW-UUy8"
   },
   "source": [
    "## Setting the trainer to the restored model\n",
    "Setting the trainer to the restored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:36.669403Z",
     "start_time": "2024-03-09T09:44:36.647025Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0FhYQQQOUPIk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:03 exp_manager:396] Experiments will be logged at /home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57\n",
      "[NeMo I 2024-03-11 08:23:03 exp_manager:842] TensorboardLogger has been set up\n",
      "/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57\n"
     ]
    }
   ],
   "source": [
    "log_dir_finetune = exp_manager(trainer_finetune, config.get(\"exp_manager\", None))\n",
    "print(log_dir_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lc3fzGYVVTyi"
   },
   "source": [
    "## Fine-tune training step\n",
    "\n",
    "When fine-tuning on a truly new dataset, we will not see such a dramatic improvement in performance. However, it should still converge a little faster than if it was trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:44:37.672150Z",
     "start_time": "2024-03-09T09:44:36.671744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:04 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:446] Dataset loaded with 130 items, total duration of  0.10 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:448] # 130 files loaded accounting to # 10 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:23:04 label_models:189] Total number of 10 found in all the manifest files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:04 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:446] Dataset loaded with 130 items, total duration of  0.10 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:448] # 130 files loaded accounting to # 10 labels\n",
      "[NeMo I 2024-03-11 08:23:04 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:446] Dataset loaded with 130 items, total duration of  0.10 hours.\n",
      "[NeMo I 2024-03-11 08:23:04 collections:448] # 130 files loaded accounting to # 10 labels\n",
      "[NeMo I 2024-03-11 08:23:04 features:289] PADDING: 16\n",
      "[NeMo I 2024-03-11 08:23:04 cloud:58] Found existing object /home/eyal/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2024-03-11 08:23:04 cloud:64] Re-using file from: /home/eyal/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo\n",
      "[NeMo I 2024-03-11 08:23:04 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:23:04 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-03-11 08:23:04 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:04 features:289] PADDING: 16\n",
      "[NeMo I 2024-03-11 08:23:05 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /home/eyal/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2024-03-11 08:23:05 modelPT:1145] Model checkpoint partially restored from pretrained checkpoint with name `titanet_large`\n",
      "[NeMo I 2024-03-11 08:23:05 modelPT:1147] The following parameters were excluded when loading from pretrained checkpoint with name `titanet_large` : ['decoder.final.weight']\n",
      "[NeMo I 2024-03-11 08:23:05 modelPT:1150] Make sure that this is what you wanted!\n"
     ]
    }
   ],
   "source": [
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=finetune_config.model, trainer=trainer_finetune)\n",
    "speaker_model.maybe_init_from_pretrained_checkpoint(finetune_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the config, we keep weights of preprocessor and encoder, and attach a new decoder as mentioned in above section to match num of classes of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:26.038303Z",
     "start_time": "2024-03-09T09:44:37.672994Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uFIOsuFYVLzr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:23:05 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints exists and is not empty.\n",
      "      rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:05 modelPT:723] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 0.0001\n",
      "        maximize: False\n",
      "        weight_decay: 0.0002\n",
      "    \n",
      "    Parameter Group 1\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 0.001\n",
      "        maximize: False\n",
      "        weight_decay: 0.0002\n",
      "    )\n",
      "[NeMo I 2024-03-11 08:23:05 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f3d70164d30>\" \n",
      "    will be used during training (effective maximum steps = 15) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1\n",
      "    min_lr: 0.0\n",
      "    max_steps: 15\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type                              | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | loss            | AngularSoftmaxLoss                | 0     \n",
      "1 | eval_loss       | AngularSoftmaxLoss                | 0     \n",
      "2 | _accuracy       | TopKClassificationAccuracy        | 0     \n",
      "3 | preprocessor    | AudioToMelSpectrogramPreprocessor | 0     \n",
      "4 | encoder         | ConvASREncoder                    | 19.4 M\n",
      "5 | decoder         | SpeakerDecoder                    | 2.8 M \n",
      "6 | _macro_accuracy | MulticlassAccuracy                | 0     \n",
      "----------------------------------------------------------------------\n",
      "22.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.497    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cd595f09a14df89d76d1f56acb17ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-11 08:23:05 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-03-11 08:23:05 nemo_logging:349] /home/eyal/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d693999168f24d128bdd9a00514cea01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-11 08:23:05 preemption:56] Preemption requires torch distributed to be initialized, disabling preemption\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2812ffbd196442dfbc3cca2ae6ddb76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 3: 'val_loss' reached 11.26189 (best 11.26189), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=11.2619-epoch=0.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8291ed0d7a544d9996fdaf9ce0375ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 6: 'val_loss' reached 4.12678 (best 4.12678), saving model to '/home/eyal/git/hakol/notebooks/nemo_experiments/ECAPA_TDNN/2024-03-11_08-21-57/checkpoints/ECAPA_TDNN--val_loss=4.1268-epoch=1.ckpt' as top 3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 47.53 GiB of which 1.38 GiB is free. Process 280006 has 15.73 GiB memory in use. Process 324255 has 14.83 GiB memory in use. Including non-PyTorch memory, this process has 15.41 GiB memory in use. Of the allocated memory 11.75 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Fine-tuning for 5 epochs¶\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer_finetune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:219\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:188\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         closure()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:266\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:145\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 145\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    148\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1270\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1234\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:161\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:231\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:116\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:164\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 164\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    167\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:103\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:142\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:128\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 128\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/utils/model_utils.py:381\u001b[0m, in \u001b[0;36mwrap_training_step\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;129m@wrapt\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_training_step\u001b[39m(wrapped, instance: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m'\u001b[39m, args, kwargs):\n\u001b[0;32m--> 381\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_dict, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m output_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output_dict:\n\u001b[1;32m    384\u001b[0m         log_dict \u001b[38;5;241m=\u001b[39m output_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/label_models.py:354\u001b[0m, in \u001b[0;36mEncDecSpeakerLabelModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    353\u001b[0m     audio_signal, audio_signal_len, labels, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 354\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_signal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_signal_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_signal_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/core/classes/common.py:1098\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m instance\u001b[38;5;241m.\u001b[39m_validate_input_types(input_types\u001b[38;5;241m=\u001b[39minput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m instance\u001b[38;5;241m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1101\u001b[0m     output_types\u001b[38;5;241m=\u001b[39moutput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, out_objects\u001b[38;5;241m=\u001b[39moutputs\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/label_models.py:348\u001b[0m, in \u001b[0;36mEncDecSpeakerLabelModel.forward\u001b[0;34m(self, input_signal, input_signal_length)\u001b[0m\n\u001b[1;32m    345\u001b[0m     processed_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec_augmentation(input_spec\u001b[38;5;241m=\u001b[39mprocessed_signal, length\u001b[38;5;241m=\u001b[39mprocessed_signal_len)\n\u001b[1;32m    347\u001b[0m encoded, length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(audio_signal\u001b[38;5;241m=\u001b[39mprocessed_signal, length\u001b[38;5;241m=\u001b[39mprocessed_signal_len)\n\u001b[0;32m--> 348\u001b[0m logits, embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, embs\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/core/classes/common.py:1098\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m instance\u001b[38;5;241m.\u001b[39m_validate_input_types(input_types\u001b[38;5;241m=\u001b[39minput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m instance\u001b[38;5;241m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1101\u001b[0m     output_types\u001b[38;5;241m=\u001b[39moutput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, out_objects\u001b[38;5;241m=\u001b[39moutputs\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/collections/asr/modules/conv_asr.py:875\u001b[0m, in \u001b[0;36mSpeakerDecoder.forward\u001b[0;34m(self, encoder_output, length)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;129m@typecheck\u001b[39m()\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_output, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 875\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m     embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layers:\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/hakol/.venv/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/tdnn_attention.py:315\u001b[0m, in \u001b[0;36mAttentivePoolLayer.forward\u001b[0;34m(self, x, length)\u001b[0m\n\u001b[1;32m    313\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, max_len)\n\u001b[1;32m    314\u001b[0m std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, max_len)\n\u001b[0;32m--> 315\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# attention statistics\u001b[39;00m\n\u001b[1;32m    318\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer(attn)  \u001b[38;5;66;03m# attention pass\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 47.53 GiB of which 1.38 GiB is free. Process 280006 has 15.73 GiB memory in use. Process 324255 has 14.83 GiB memory in use. Including non-PyTorch memory, this process has 15.41 GiB memory in use. Of the allocated memory 11.75 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "## Fine-tuning for 5 epochs¶\n",
    "trainer_finetune.fit(speaker_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: Add more data augmentation and dropout while finetuning on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DNidtl4VplU"
   },
   "source": [
    "# Saving .nemo file\n",
    "Now we can save the whole config and model parameters in a single .nemo and we can anytime restore from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:26.364031Z",
     "start_time": "2024-03-09T09:45:26.039218Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "am5wej6-VdZW"
   },
   "outputs": [],
   "source": [
    "saved_model = 'titanet-small-finetune.nemo'\n",
    "restored_model.save_to(os.path.join(log_dir_finetune, '..',saved_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:26.597665Z",
     "start_time": "2024-03-09T09:45:26.364946Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WnBhFJefV-Pf"
   },
   "outputs": [],
   "source": [
    "!ls {log_dir_finetune}/.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:26.817633Z",
     "start_time": "2024-03-09T09:45:26.599670Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kVx1hNP_V_iz"
   },
   "outputs": [],
   "source": [
    "# restore from a save model\n",
    "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', saved_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80tLWTN40uaB"
   },
   "source": [
    "# Speaker Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VciRUIRz0y6P"
   },
   "source": [
    "Training for a speaker verification model is almost the same as the speaker recognition model with a change in the loss function. Angular Loss is a better function to train for a speaker verification model as the model is trained in an end-to-end manner with loss optimizing for embeddings cluster to be far from each other for different speaker by maximizing the angle between these clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULTjBuFI19Js"
   },
   "source": [
    "To train for verification we just need to toggle `angular` flag in `config.model.decoder.params.angular = True` else set it to `False` to train with cross-entropy loss for identification purposes. \n",
    "Once we set this, the loss will be changed to angular loss and we can follow the above steps to the model.\n",
    "Note the scale and margin values to be set for the loss function are present at `config.model.loss.scale` and `config.model.loss.margin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcKiNEY032-t"
   },
   "source": [
    "## Extract Speaker Embeddings\n",
    "Once you have a trained model or use one of our pretrained nemo checkpoints to get speaker embeddings for any speaker.\n",
    "\n",
    "To demonstrate this we shall use `nemo_asr.models.EncDecSpeakerLabelModel` with say 5 audio_samples from our dev manifest set. This model is specifically for inference purposes to extract embeddings from a trained `.nemo` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.051067Z",
     "start_time": "2024-03-09T09:45:26.818964Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uXEzKMHf3r6-"
   },
   "outputs": [],
   "source": [
    "verification_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', saved_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-XiLHMQ8BIk"
   },
   "source": [
    "Now, we need to pass the necessary manifest_filepath and params to set up the data loader for extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.303454Z",
     "start_time": "2024-03-09T09:45:27.052085Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lk2vsDJk9PS8"
   },
   "outputs": [],
   "source": [
    "!head -5 {validation_manifest} > embeddings_manifest.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.312585Z",
     "start_time": "2024-03-09T09:45:27.304606Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DEd5poCr9yrP"
   },
   "outputs": [],
   "source": [
    "config.model.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.316831Z",
     "start_time": "2024-03-09T09:45:27.313662Z"
    }
   },
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.speaker_utils import embedding_normalize\n",
    "from tqdm import  tqdm\n",
    "try:\n",
    "    from torch.cuda.amp import autocast\n",
    "except ImportError:\n",
    "    from contextlib import contextmanager\n",
    "\n",
    "    @contextmanager\n",
    "    def autocast(enabled=None):\n",
    "        yield\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.324522Z",
     "start_time": "2024-03-09T09:45:27.317964Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JIHok6LD8g0F"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(speaker_model, manifest_file, batch_size=1, embedding_dir='./', device='cuda'):\n",
    "    test_config = OmegaConf.create(\n",
    "        dict(\n",
    "            manifest_filepath=manifest_file,\n",
    "            sample_rate=16000,\n",
    "            labels=None,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            time_length=20,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    speaker_model.setup_test_data(test_config)\n",
    "    speaker_model = speaker_model.to(device)\n",
    "    speaker_model.eval()\n",
    "\n",
    "    all_embs=[]\n",
    "    out_embeddings = {}\n",
    "           \n",
    "    for test_batch in tqdm(speaker_model.test_dataloader()):\n",
    "        test_batch = [x.to(device) for x in test_batch]\n",
    "        audio_signal, audio_signal_len, labels, slices = test_batch\n",
    "        with autocast():\n",
    "            _, embs = speaker_model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
    "            emb_shape = embs.shape[-1]\n",
    "            embs = embs.view(-1, emb_shape)\n",
    "            all_embs.extend(embs.cpu().detach().numpy())\n",
    "        del test_batch\n",
    "\n",
    "    all_embs = np.asarray(all_embs)\n",
    "    all_embs = embedding_normalize(all_embs)\n",
    "    with open(manifest_file, 'r') as manifest:\n",
    "        for i, line in enumerate(manifest.readlines()):\n",
    "            line = line.strip()\n",
    "            dic = json.loads(line)\n",
    "            uniq_name = '@'.join(dic['audio_filepath'].split('/')[-3:])\n",
    "            out_embeddings[uniq_name] = all_embs[i]\n",
    "\n",
    "    embedding_dir = os.path.join(embedding_dir, 'embeddings')\n",
    "    if not os.path.exists(embedding_dir):\n",
    "        os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "    prefix = manifest_file.split('/')[-1].rsplit('.', 1)[-2]\n",
    "\n",
    "    name = os.path.join(embedding_dir, prefix)\n",
    "    embeddings_file = name + '_embeddings.pkl'\n",
    "    pkl.dump(out_embeddings, open(embeddings_file, 'wb'))\n",
    "    print(\"Saved embedding files to {}\".format(embedding_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.477539Z",
     "start_time": "2024-03-09T09:45:27.325391Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "u2FRecqD-ln5"
   },
   "outputs": [],
   "source": [
    "manifest_filepath = os.path.join(NEMO_ROOT,'embeddings_manifest.json')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "get_embeddings(verification_model, manifest_filepath, batch_size=64,embedding_dir='./', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfjXPsjzDOgr"
   },
   "source": [
    "Embeddings are stored in dict structure with key-value pair, key being uniq_name generated based on audio_filepath of the sample present in manifest_file in `embedding_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.728849Z",
     "start_time": "2024-03-09T09:45:27.478425Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hmTeSR6jD28k"
   },
   "outputs": [],
   "source": [
    "!ls ./embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:45:27.732798Z",
     "start_time": "2024-03-09T09:45:27.730185Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Speaker_Recogniton_Verification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
